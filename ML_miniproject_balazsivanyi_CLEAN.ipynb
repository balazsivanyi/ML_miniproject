{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_miniproject_balazsivanyi_CLEAN",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1omRg78ZNmYSQUceIdijkzTVoImhz6E_z",
      "authorship_tag": "ABX9TyPTxUarx9hIiitdlDH4Io4w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/balazsivanyi/ML_miniproject/blob/main/ML_miniproject_balazsivanyi_CLEAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Session-aware music recommendation ML algorithm on Spotify datasets"
      ],
      "metadata": {
        "id": "npmih46DIpSV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Balazs Andras Ivanyi, Aalborg University Copenhagen\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "QrDbYVtUI77X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Brief description of project"
      ],
      "metadata": {
        "id": "3mP4OEkvJNJj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Originally, I wanted to create an application that would have been a session-based music recommendation algorithm, taking one song as an input, and producing a playlist (a set of songs) as an output. It was identified in recent studies that deep learning-based “neural” approaches tend to perform worse in classical session-based recommendation tasks than simpler algorithms, such as kNNs. The purpose was to evaluate a kNN session-based recommendation, and compare to state of the art DL-based recommendation system benchmarks, on Spotify's Million Playlist Dataset. However, I realised that it would be a more suitable apporach to start with a less complex implementation, and focus on music streaming skip prediction first."
      ],
      "metadata": {
        "id": "Oc_RSvIIJnVl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu3-BFWaizwe"
      },
      "source": [
        "## Install and setup phase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxexENR4i3mr"
      },
      "source": [
        "Importing libraries and dependencies for the project. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-qrWCzctd4w"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np \n",
        "import matplotlib as mpl \n",
        "import matplotlib.pyplot as plt \n",
        "import pandas as pd\n",
        "import seaborn as sb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuvokbufU9Ln"
      },
      "source": [
        "Setting up final dataframe from Spotify's skip prediction [challange](https://www.aicrowd.com/challenges/spotify-sequential-skip-prediction-challenge). The dataset is broken down to training and test sets, but they are considerably heavy (60G and 14G respectively). Alongside these files, the track data's audio features are located in separate files. So first I merged the two trackdata-files, and I used only one segment of the user interaction log data from total 60G."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttPcDcyjdvIN",
        "outputId": "c1539df5-0792-4650-e089-a97608d2fbfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of tracks: 3706388\n"
          ]
        }
      ],
      "source": [
        "#loading in track audio features data\n",
        "trackData1 = pd.read_csv('/content/drive/MyDrive/ML_miniproject_2021/tf_000000000000.csv')                           \n",
        "trackData2 = pd.read_csv('/content/drive/MyDrive/ML_miniproject_2021/tf_000000000001.csv')\n",
        "\n",
        "#combine into one dataframe                               \n",
        "trackData = pd.concat([trackData1, trackData2], ignore_index=True)\n",
        "print('Total number of tracks: {}'.format(len(trackData)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znyOarg7BU-f",
        "outputId": "82ee2bf0-2296-4e85-92c1-76b9a3e8cb22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of user interaction logs: 3105679\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "# Spotify streaming session data\n",
        "userData = pd.read_csv('/content/drive/MyDrive/ML_miniproject_2021/log_0_20180815_000000000000.csv')\n",
        "print('Total number of user interaction logs: {}'.format(len(userData)))\n",
        "\n",
        "#check if streaming data is not missing any songs\n",
        "set(userData.track_id_clean).issubset(set(trackData.track_id))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tracks' audio features are from Spotify API acoustic features [database](https://developer.spotify.com/documentation/web-api/reference/#/operations/get-several-audio-features). These describe the acoustic features of each song in the streaming session, and these features can be analysed if they can be used to predict skipping behaviours. The acoustic dataset's features can be seen printed below, and each feature's values can be found on Spotify's API documentation."
      ],
      "metadata": {
        "id": "ciYB_ojPLToU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZHCpiT6f4Uq",
        "outputId": "9729ce96-2036-415a-f0d1-3df71a5d046d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['track_id', 'duration', 'release_year', 'us_popularity_estimate',\n",
              "       'acousticness', 'beat_strength', 'bounciness', 'danceability',\n",
              "       'dyn_range_mean', 'energy', 'flatness', 'instrumentalness', 'key',\n",
              "       'liveness', 'loudness', 'mechanism', 'mode', 'organism', 'speechiness',\n",
              "       'tempo', 'time_signature', 'valence', 'acoustic_vector_0',\n",
              "       'acoustic_vector_1', 'acoustic_vector_2', 'acoustic_vector_3',\n",
              "       'acoustic_vector_4', 'acoustic_vector_5', 'acoustic_vector_6',\n",
              "       'acoustic_vector_7'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "#check columns of track dataframe\n",
        "trackData.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The user interaction dataset presents user's interactions via a music streaming session on Spotify. Each session has its unique identifier, the length of the streaming session, the played tracks in that session, and these track's position in the session:\n",
        "```\n",
        "session_id, session_length, track_id_clean, session_position\n",
        "```\n",
        "Moreover, the dataset contains wether that specific track was skipped in the streaming session. It also shows, if the song was skipped right after, shortly after, or way after it started playing. All these features, as well as the complete set of features are described on the skip prediction challange's [website](https://aicrowd-production.s3.eu-central-1.amazonaws.com/dataset_files/challenge_204/7dcfad42-65c6-4481-abe8-5a44339fa305_Dataset%20Description.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAJ6IZH6GWKDCCDFAQ%2F20220103%2Feu-central-1%2Fs3%2Faws4_request&X-Amz-Date=20220103T140044Z&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=9a5942aa3186479c13868238ed4466eab638c051e77d1bd699e8535268e559f4).\n"
      ],
      "metadata": {
        "id": "Ij2RxUVPNRWi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxHKXb_-hKbO",
        "outputId": "3fdf9d27-3032-48ee-9e6c-0ba906a92167"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['session_id', 'session_position', 'session_length', 'track_id_clean',\n",
              "       'skip_1', 'skip_2', 'skip_3', 'not_skipped', 'context_switch',\n",
              "       'no_pause_before_play', 'short_pause_before_play',\n",
              "       'long_pause_before_play', 'hist_user_behavior_n_seekfwd',\n",
              "       'hist_user_behavior_n_seekback', 'hist_user_behavior_is_shuffle',\n",
              "       'hour_of_day', 'date', 'premium', 'context_type',\n",
              "       'hist_user_behavior_reason_start', 'hist_user_behavior_reason_end'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "#check columns of user interaction dataframe\n",
        "userData.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then the two dataframes were merged by their shared column, the track IDs."
      ],
      "metadata": {
        "id": "WENtCCnoeH-f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_4Vak2bsLcI",
        "outputId": "9ac366dd-c2ea-414a-a4f9-7943e8775b3d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['session_id', 'session_position', 'session_length', 'track_id',\n",
              "       'skip_1', 'skip_2', 'skip_3', 'not_skipped', 'context_switch',\n",
              "       'no_pause_before_play', 'short_pause_before_play',\n",
              "       'long_pause_before_play', 'hist_user_behavior_n_seekfwd',\n",
              "       'hist_user_behavior_n_seekback', 'hist_user_behavior_is_shuffle',\n",
              "       'hour_of_day', 'date', 'premium', 'context_type',\n",
              "       'hist_user_behavior_reason_start', 'hist_user_behavior_reason_end',\n",
              "       'duration', 'release_year', 'us_popularity_estimate', 'acousticness',\n",
              "       'beat_strength', 'bounciness', 'danceability', 'dyn_range_mean',\n",
              "       'energy', 'flatness', 'instrumentalness', 'key', 'liveness', 'loudness',\n",
              "       'mechanism', 'mode', 'organism', 'speechiness', 'tempo',\n",
              "       'time_signature', 'valence', 'acoustic_vector_0', 'acoustic_vector_1',\n",
              "       'acoustic_vector_2', 'acoustic_vector_3', 'acoustic_vector_4',\n",
              "       'acoustic_vector_5', 'acoustic_vector_6', 'acoustic_vector_7'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "#renaming columns to match in the two dataframes\n",
        "userData = userData.rename(columns={'track_id_clean': 'track_id'})\n",
        "\n",
        "#matching and merging dataframes by track_id column\n",
        "mergedData = userData.merge(trackData, how='left', on=\"track_id\")\n",
        "\n",
        "#remapping major and minor from string to binary values\n",
        "mergedData['mode'] = mergedData['mode'].map({'major': 1, 'minor': 0})\n",
        "\n",
        "mergedData.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 933
        },
        "id": "ZR7-mFH7V4Vf",
        "outputId": "c76c0ded-1b80-4942-d516-fa6db359a890"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-10d33a20-8ba6-4d6e-a6f4-81676ff37940\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>session_id</th>\n",
              "      <th>session_position</th>\n",
              "      <th>session_length</th>\n",
              "      <th>track_id</th>\n",
              "      <th>skip_1</th>\n",
              "      <th>skip_2</th>\n",
              "      <th>skip_3</th>\n",
              "      <th>not_skipped</th>\n",
              "      <th>context_switch</th>\n",
              "      <th>no_pause_before_play</th>\n",
              "      <th>short_pause_before_play</th>\n",
              "      <th>long_pause_before_play</th>\n",
              "      <th>hist_user_behavior_n_seekfwd</th>\n",
              "      <th>hist_user_behavior_n_seekback</th>\n",
              "      <th>hist_user_behavior_is_shuffle</th>\n",
              "      <th>hour_of_day</th>\n",
              "      <th>date</th>\n",
              "      <th>premium</th>\n",
              "      <th>context_type</th>\n",
              "      <th>hist_user_behavior_reason_start</th>\n",
              "      <th>hist_user_behavior_reason_end</th>\n",
              "      <th>duration</th>\n",
              "      <th>release_year</th>\n",
              "      <th>us_popularity_estimate</th>\n",
              "      <th>acousticness</th>\n",
              "      <th>beat_strength</th>\n",
              "      <th>bounciness</th>\n",
              "      <th>danceability</th>\n",
              "      <th>dyn_range_mean</th>\n",
              "      <th>energy</th>\n",
              "      <th>flatness</th>\n",
              "      <th>instrumentalness</th>\n",
              "      <th>key</th>\n",
              "      <th>liveness</th>\n",
              "      <th>loudness</th>\n",
              "      <th>mechanism</th>\n",
              "      <th>mode</th>\n",
              "      <th>organism</th>\n",
              "      <th>speechiness</th>\n",
              "      <th>tempo</th>\n",
              "      <th>time_signature</th>\n",
              "      <th>valence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>31_0000b0c5-94b8-426b-87e2-ef81510b9b17</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>t_86abc9b1-2a71-41d8-ab97-ac97ea20276a</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "      <td>8</td>\n",
              "      <td>2018-08-14</td>\n",
              "      <td>True</td>\n",
              "      <td>user_collection</td>\n",
              "      <td>fwdbtn</td>\n",
              "      <td>fwdbtn</td>\n",
              "      <td>200.546677</td>\n",
              "      <td>2006</td>\n",
              "      <td>99.997576</td>\n",
              "      <td>0.000059</td>\n",
              "      <td>0.205253</td>\n",
              "      <td>0.191648</td>\n",
              "      <td>0.390629</td>\n",
              "      <td>4.778084</td>\n",
              "      <td>0.963439</td>\n",
              "      <td>0.930136</td>\n",
              "      <td>6.890018e-02</td>\n",
              "      <td>10</td>\n",
              "      <td>0.140413</td>\n",
              "      <td>-4.378</td>\n",
              "      <td>0.777542</td>\n",
              "      <td>0</td>\n",
              "      <td>0.157301</td>\n",
              "      <td>0.077345</td>\n",
              "      <td>167.065002</td>\n",
              "      <td>4</td>\n",
              "      <td>0.363948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>31_0000b0c5-94b8-426b-87e2-ef81510b9b17</td>\n",
              "      <td>2</td>\n",
              "      <td>20</td>\n",
              "      <td>t_33a133e6-240c-467d-a5c5-a6729a545cc2</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "      <td>8</td>\n",
              "      <td>2018-08-14</td>\n",
              "      <td>True</td>\n",
              "      <td>user_collection</td>\n",
              "      <td>fwdbtn</td>\n",
              "      <td>fwdbtn</td>\n",
              "      <td>246.519730</td>\n",
              "      <td>2015</td>\n",
              "      <td>97.391548</td>\n",
              "      <td>0.000445</td>\n",
              "      <td>0.197883</td>\n",
              "      <td>0.172790</td>\n",
              "      <td>0.228356</td>\n",
              "      <td>4.468809</td>\n",
              "      <td>0.890128</td>\n",
              "      <td>0.968709</td>\n",
              "      <td>2.377608e-01</td>\n",
              "      <td>11</td>\n",
              "      <td>0.307806</td>\n",
              "      <td>-2.373</td>\n",
              "      <td>0.334852</td>\n",
              "      <td>0</td>\n",
              "      <td>0.470331</td>\n",
              "      <td>0.060110</td>\n",
              "      <td>138.798996</td>\n",
              "      <td>5</td>\n",
              "      <td>0.475900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>31_0000b0c5-94b8-426b-87e2-ef81510b9b17</td>\n",
              "      <td>3</td>\n",
              "      <td>20</td>\n",
              "      <td>t_cd87b117-d9d0-4562-b469-65ae0e88f8f5</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "      <td>8</td>\n",
              "      <td>2018-08-14</td>\n",
              "      <td>True</td>\n",
              "      <td>user_collection</td>\n",
              "      <td>fwdbtn</td>\n",
              "      <td>fwdbtn</td>\n",
              "      <td>230.213333</td>\n",
              "      <td>2015</td>\n",
              "      <td>99.961404</td>\n",
              "      <td>0.090172</td>\n",
              "      <td>0.367402</td>\n",
              "      <td>0.333184</td>\n",
              "      <td>0.541190</td>\n",
              "      <td>5.807304</td>\n",
              "      <td>0.643394</td>\n",
              "      <td>0.982674</td>\n",
              "      <td>9.846400e-09</td>\n",
              "      <td>3</td>\n",
              "      <td>0.152956</td>\n",
              "      <td>-5.517</td>\n",
              "      <td>0.627907</td>\n",
              "      <td>0</td>\n",
              "      <td>0.270725</td>\n",
              "      <td>0.036610</td>\n",
              "      <td>90.060997</td>\n",
              "      <td>4</td>\n",
              "      <td>0.491455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>31_0000b0c5-94b8-426b-87e2-ef81510b9b17</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>t_de6bfde1-10b3-4984-add7-b41050bc9353</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "      <td>8</td>\n",
              "      <td>2018-08-14</td>\n",
              "      <td>True</td>\n",
              "      <td>user_collection</td>\n",
              "      <td>fwdbtn</td>\n",
              "      <td>fwdbtn</td>\n",
              "      <td>207.786667</td>\n",
              "      <td>2017</td>\n",
              "      <td>99.999173</td>\n",
              "      <td>0.422664</td>\n",
              "      <td>0.268346</td>\n",
              "      <td>0.280567</td>\n",
              "      <td>0.277216</td>\n",
              "      <td>5.783823</td>\n",
              "      <td>0.393620</td>\n",
              "      <td>1.011296</td>\n",
              "      <td>6.704910e-07</td>\n",
              "      <td>8</td>\n",
              "      <td>0.095243</td>\n",
              "      <td>-8.903</td>\n",
              "      <td>0.169231</td>\n",
              "      <td>1</td>\n",
              "      <td>0.659099</td>\n",
              "      <td>0.033571</td>\n",
              "      <td>86.777000</td>\n",
              "      <td>3</td>\n",
              "      <td>0.226621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>31_0000b0c5-94b8-426b-87e2-ef81510b9b17</td>\n",
              "      <td>5</td>\n",
              "      <td>20</td>\n",
              "      <td>t_01d7104d-d28c-4c56-9012-d22ef2b8bdc9</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "      <td>8</td>\n",
              "      <td>2018-08-14</td>\n",
              "      <td>True</td>\n",
              "      <td>user_collection</td>\n",
              "      <td>fwdbtn</td>\n",
              "      <td>trackdone</td>\n",
              "      <td>195.518997</td>\n",
              "      <td>2018</td>\n",
              "      <td>99.999788</td>\n",
              "      <td>0.002812</td>\n",
              "      <td>0.496197</td>\n",
              "      <td>0.445518</td>\n",
              "      <td>0.630482</td>\n",
              "      <td>6.607257</td>\n",
              "      <td>0.694285</td>\n",
              "      <td>1.031873</td>\n",
              "      <td>1.266271e-10</td>\n",
              "      <td>11</td>\n",
              "      <td>0.071866</td>\n",
              "      <td>-6.257</td>\n",
              "      <td>0.773723</td>\n",
              "      <td>0</td>\n",
              "      <td>0.160015</td>\n",
              "      <td>0.025284</td>\n",
              "      <td>97.004997</td>\n",
              "      <td>4</td>\n",
              "      <td>0.215933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>31_0000b0c5-94b8-426b-87e2-ef81510b9b17</td>\n",
              "      <td>6</td>\n",
              "      <td>20</td>\n",
              "      <td>t_ff674955-20ad-48bf-8494-d5fbe9dd7fac</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "      <td>8</td>\n",
              "      <td>2018-08-14</td>\n",
              "      <td>True</td>\n",
              "      <td>user_collection</td>\n",
              "      <td>trackdone</td>\n",
              "      <td>fwdbtn</td>\n",
              "      <td>245.053329</td>\n",
              "      <td>2011</td>\n",
              "      <td>93.858501</td>\n",
              "      <td>0.068239</td>\n",
              "      <td>0.349181</td>\n",
              "      <td>0.350332</td>\n",
              "      <td>0.602128</td>\n",
              "      <td>6.290171</td>\n",
              "      <td>0.795717</td>\n",
              "      <td>0.963909</td>\n",
              "      <td>1.196051e-01</td>\n",
              "      <td>0</td>\n",
              "      <td>0.149576</td>\n",
              "      <td>-3.657</td>\n",
              "      <td>0.570766</td>\n",
              "      <td>0</td>\n",
              "      <td>0.307326</td>\n",
              "      <td>0.103283</td>\n",
              "      <td>126.059998</td>\n",
              "      <td>4</td>\n",
              "      <td>0.264787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>31_0000b0c5-94b8-426b-87e2-ef81510b9b17</td>\n",
              "      <td>7</td>\n",
              "      <td>20</td>\n",
              "      <td>t_0478077e-cc90-48f5-a989-21714d69151d</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "      <td>8</td>\n",
              "      <td>2018-08-14</td>\n",
              "      <td>True</td>\n",
              "      <td>user_collection</td>\n",
              "      <td>fwdbtn</td>\n",
              "      <td>fwdbtn</td>\n",
              "      <td>174.226669</td>\n",
              "      <td>2018</td>\n",
              "      <td>99.895486</td>\n",
              "      <td>0.590150</td>\n",
              "      <td>0.543267</td>\n",
              "      <td>0.535417</td>\n",
              "      <td>0.537070</td>\n",
              "      <td>7.862614</td>\n",
              "      <td>0.596715</td>\n",
              "      <td>1.040544</td>\n",
              "      <td>1.141131e-05</td>\n",
              "      <td>7</td>\n",
              "      <td>0.100791</td>\n",
              "      <td>-7.641</td>\n",
              "      <td>0.532663</td>\n",
              "      <td>0</td>\n",
              "      <td>0.532297</td>\n",
              "      <td>0.071800</td>\n",
              "      <td>79.959999</td>\n",
              "      <td>4</td>\n",
              "      <td>0.388015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>31_0000b0c5-94b8-426b-87e2-ef81510b9b17</td>\n",
              "      <td>8</td>\n",
              "      <td>20</td>\n",
              "      <td>t_3c70d8ac-b601-4f8e-be57-cfdb7f119183</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "      <td>8</td>\n",
              "      <td>2018-08-14</td>\n",
              "      <td>True</td>\n",
              "      <td>user_collection</td>\n",
              "      <td>fwdbtn</td>\n",
              "      <td>fwdbtn</td>\n",
              "      <td>227.533325</td>\n",
              "      <td>2017</td>\n",
              "      <td>99.999812</td>\n",
              "      <td>0.627452</td>\n",
              "      <td>0.496016</td>\n",
              "      <td>0.589231</td>\n",
              "      <td>0.670188</td>\n",
              "      <td>9.350102</td>\n",
              "      <td>0.654282</td>\n",
              "      <td>1.025306</td>\n",
              "      <td>1.021170e-06</td>\n",
              "      <td>4</td>\n",
              "      <td>0.071018</td>\n",
              "      <td>-5.944</td>\n",
              "      <td>0.820244</td>\n",
              "      <td>1</td>\n",
              "      <td>0.461523</td>\n",
              "      <td>0.153234</td>\n",
              "      <td>180.024002</td>\n",
              "      <td>4</td>\n",
              "      <td>0.437593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>31_0000b0c5-94b8-426b-87e2-ef81510b9b17</td>\n",
              "      <td>9</td>\n",
              "      <td>20</td>\n",
              "      <td>t_048f0e89-bdcb-4d33-bcea-a4f4c3591cc4</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "      <td>8</td>\n",
              "      <td>2018-08-14</td>\n",
              "      <td>True</td>\n",
              "      <td>user_collection</td>\n",
              "      <td>fwdbtn</td>\n",
              "      <td>fwdbtn</td>\n",
              "      <td>268.866669</td>\n",
              "      <td>2017</td>\n",
              "      <td>99.999286</td>\n",
              "      <td>0.053429</td>\n",
              "      <td>0.461715</td>\n",
              "      <td>0.462736</td>\n",
              "      <td>0.588504</td>\n",
              "      <td>7.273110</td>\n",
              "      <td>0.730975</td>\n",
              "      <td>1.014365</td>\n",
              "      <td>1.496789e-10</td>\n",
              "      <td>2</td>\n",
              "      <td>0.307783</td>\n",
              "      <td>-6.343</td>\n",
              "      <td>0.510145</td>\n",
              "      <td>1</td>\n",
              "      <td>0.348434</td>\n",
              "      <td>0.086839</td>\n",
              "      <td>87.907997</td>\n",
              "      <td>4</td>\n",
              "      <td>0.190725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>31_0000b0c5-94b8-426b-87e2-ef81510b9b17</td>\n",
              "      <td>10</td>\n",
              "      <td>20</td>\n",
              "      <td>t_52fc9bcf-ce50-43fc-9498-c2c8421a33e7</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "      <td>8</td>\n",
              "      <td>2018-08-14</td>\n",
              "      <td>True</td>\n",
              "      <td>user_collection</td>\n",
              "      <td>fwdbtn</td>\n",
              "      <td>fwdbtn</td>\n",
              "      <td>222.653336</td>\n",
              "      <td>2018</td>\n",
              "      <td>99.999710</td>\n",
              "      <td>0.044075</td>\n",
              "      <td>0.575090</td>\n",
              "      <td>0.579509</td>\n",
              "      <td>0.736817</td>\n",
              "      <td>8.417690</td>\n",
              "      <td>0.636002</td>\n",
              "      <td>1.039552</td>\n",
              "      <td>6.660252e-05</td>\n",
              "      <td>11</td>\n",
              "      <td>0.350031</td>\n",
              "      <td>-4.546</td>\n",
              "      <td>0.771015</td>\n",
              "      <td>0</td>\n",
              "      <td>0.164889</td>\n",
              "      <td>0.043701</td>\n",
              "      <td>105.004997</td>\n",
              "      <td>4</td>\n",
              "      <td>0.564775</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-10d33a20-8ba6-4d6e-a6f4-81676ff37940')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-10d33a20-8ba6-4d6e-a6f4-81676ff37940 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-10d33a20-8ba6-4d6e-a6f4-81676ff37940');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                session_id  ...   valence\n",
              "0  31_0000b0c5-94b8-426b-87e2-ef81510b9b17  ...  0.363948\n",
              "1  31_0000b0c5-94b8-426b-87e2-ef81510b9b17  ...  0.475900\n",
              "2  31_0000b0c5-94b8-426b-87e2-ef81510b9b17  ...  0.491455\n",
              "3  31_0000b0c5-94b8-426b-87e2-ef81510b9b17  ...  0.226621\n",
              "4  31_0000b0c5-94b8-426b-87e2-ef81510b9b17  ...  0.215933\n",
              "5  31_0000b0c5-94b8-426b-87e2-ef81510b9b17  ...  0.264787\n",
              "6  31_0000b0c5-94b8-426b-87e2-ef81510b9b17  ...  0.388015\n",
              "7  31_0000b0c5-94b8-426b-87e2-ef81510b9b17  ...  0.437593\n",
              "8  31_0000b0c5-94b8-426b-87e2-ef81510b9b17  ...  0.190725\n",
              "9  31_0000b0c5-94b8-426b-87e2-ef81510b9b17  ...  0.564775\n",
              "\n",
              "[10 rows x 42 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "#deleting last columns, which won't be used for feature engineering\n",
        "mergedData = mergedData.drop(['acoustic_vector_0', 'acoustic_vector_1', 'acoustic_vector_2', 'acoustic_vector_3', 'acoustic_vector_4', 'acoustic_vector_5', 'acoustic_vector_6', 'acoustic_vector_7'], axis=1)\n",
        "\n",
        "#displaying an excerpet of the dataframe for checking\n",
        "mergedData.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ylq1CUjjQSH",
        "outputId": "0569be09-0b1a-42e0-d7f9-8214c74bce6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 3105679 entries, 0 to 3105678\n",
            "Data columns (total 42 columns):\n",
            " #   Column                           Dtype  \n",
            "---  ------                           -----  \n",
            " 0   session_id                       object \n",
            " 1   session_position                 int64  \n",
            " 2   session_length                   int64  \n",
            " 3   track_id                         object \n",
            " 4   skip_1                           bool   \n",
            " 5   skip_2                           bool   \n",
            " 6   skip_3                           bool   \n",
            " 7   not_skipped                      bool   \n",
            " 8   context_switch                   int64  \n",
            " 9   no_pause_before_play             int64  \n",
            " 10  short_pause_before_play          int64  \n",
            " 11  long_pause_before_play           int64  \n",
            " 12  hist_user_behavior_n_seekfwd     int64  \n",
            " 13  hist_user_behavior_n_seekback    int64  \n",
            " 14  hist_user_behavior_is_shuffle    bool   \n",
            " 15  hour_of_day                      int64  \n",
            " 16  date                             object \n",
            " 17  premium                          bool   \n",
            " 18  context_type                     object \n",
            " 19  hist_user_behavior_reason_start  object \n",
            " 20  hist_user_behavior_reason_end    object \n",
            " 21  duration                         float64\n",
            " 22  release_year                     int64  \n",
            " 23  us_popularity_estimate           float64\n",
            " 24  acousticness                     float64\n",
            " 25  beat_strength                    float64\n",
            " 26  bounciness                       float64\n",
            " 27  danceability                     float64\n",
            " 28  dyn_range_mean                   float64\n",
            " 29  energy                           float64\n",
            " 30  flatness                         float64\n",
            " 31  instrumentalness                 float64\n",
            " 32  key                              int64  \n",
            " 33  liveness                         float64\n",
            " 34  loudness                         float64\n",
            " 35  mechanism                        float64\n",
            " 36  mode                             int64  \n",
            " 37  organism                         float64\n",
            " 38  speechiness                      float64\n",
            " 39  tempo                            float64\n",
            " 40  time_signature                   int64  \n",
            " 41  valence                          float64\n",
            "dtypes: bool(6), float64(17), int64(13), object(6)\n",
            "memory usage: 894.5+ MB\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "session_id                         0\n",
              "session_position                   0\n",
              "session_length                     0\n",
              "track_id                           0\n",
              "skip_1                             0\n",
              "skip_2                             0\n",
              "skip_3                             0\n",
              "not_skipped                        0\n",
              "context_switch                     0\n",
              "no_pause_before_play               0\n",
              "short_pause_before_play            0\n",
              "long_pause_before_play             0\n",
              "hist_user_behavior_n_seekfwd       0\n",
              "hist_user_behavior_n_seekback      0\n",
              "hist_user_behavior_is_shuffle      0\n",
              "hour_of_day                        0\n",
              "date                               0\n",
              "premium                            0\n",
              "context_type                       0\n",
              "hist_user_behavior_reason_start    0\n",
              "hist_user_behavior_reason_end      0\n",
              "duration                           0\n",
              "release_year                       0\n",
              "us_popularity_estimate             0\n",
              "acousticness                       0\n",
              "beat_strength                      0\n",
              "bounciness                         0\n",
              "danceability                       0\n",
              "dyn_range_mean                     0\n",
              "energy                             0\n",
              "flatness                           0\n",
              "instrumentalness                   0\n",
              "key                                0\n",
              "liveness                           0\n",
              "loudness                           0\n",
              "mechanism                          0\n",
              "mode                               0\n",
              "organism                           0\n",
              "speechiness                        0\n",
              "tempo                              0\n",
              "time_signature                     0\n",
              "valence                            0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "\n",
        "#check details of merged dataframe\n",
        "mergedData.info()\n",
        "mergedData.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two dataframes were succesfully merged without any empty datapoints in the merged dataframe. So the inital exploration of data and preliminary feature engineeering could begin."
      ],
      "metadata": {
        "id": "VLn1FYsPefGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np \n",
        "import matplotlib as mpl \n",
        "import matplotlib.pyplot as plt \n",
        "import pandas as pd\n",
        "import seaborn as sb\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "Tn2XdOszN6IU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfiimYmNqagb"
      },
      "source": [
        "## Exploratory Data Analysis & Feature engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I first checked what is the ratio of skipped and not skipped songs in the dataset. As the balance is skewed with almost twice as more skipped songs, I needed to balance this out later for training. Also, I was curios how many individual listening sessions are included in the dataset, which was 188450."
      ],
      "metadata": {
        "id": "jBbEY50JfLke"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIMUlDQtmIQA",
        "outputId": "764f48ed-8a7e-4790-cbca-a47bd9bb0cb3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False    2021717\n",
              "True     1083962\n",
              "Name: not_skipped, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "#number of skipped songs\n",
        "mergedData.not_skipped.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXcUw5Ytmpey",
        "outputId": "a23f93fe-9847-4f33-9894-bf06e487f8af"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "188450"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "#number of different user listening sessions\n",
        "mergedData.session_id.nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEOIJOg3pENp"
      },
      "source": [
        "### Pairwise plot of audio feautres"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![img](content/drive/MyDrive/ML_miniproject_2021/pairwise_plot.png)\n"
      ],
      "metadata": {
        "id": "XElh6s7fjRKd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wg_UDn2bnWuV"
      },
      "outputs": [],
      "source": [
        "#audio_features = ['duration', 'release_year', 'us_popularity_estimate', 'acousticness', 'beat_strength', 'bounciness', 'danceability', 'dyn_range_mean',\n",
        "#'energy', 'flatness', 'instrumentalness', 'key', 'liveness', 'loudness', 'mechanism', 'mode', 'organism', 'speechiness', 'tempo',\n",
        "#'time_signature', 'valence', 'not_skipped']\n",
        "#sb.pairplot(data = balancedData.sample(frac=.0001, replace=False, random_state=7)[audio_features], hue='not_skipped', palette='Set2', height=2, plot_kws={\"s\":7});\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I plotted a pairwise plot of the acoustic features only, so I could identify meaningful trends and correlations between different acoustic features. There were some correlations, such as between dancebility and bounciness, but I didn't find them sufficient enough to use for dimensionality reduction. The acoustic features could not be used alone to draw any trends if songs were skipped or not, either. "
      ],
      "metadata": {
        "id": "ZdNjZHlggEuQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYYZnMELocX5"
      },
      "source": [
        "### Minimum-Redundancy-Maximum-Relevance"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As k-nearest neigbour (kNN) models are time-sensitive to big datasets when training, and with a high number of features they can overfit/underfit, I further implemented dimensionality reduction. I used the Minimum-Redundancy-Maximum-Relevance algorithm, which gained more interest in the recent years, for its effectiveness and [simplicity](https://eng.uber.com/research/maximum-relevance-and-minimum-redundancy-feature-selection-methods-for-a-marketing-machine-learning-platform/). \n",
        "\n",
        "Essentially, it was designed to find the smallest relevant subset of features for a specific Machine Learning problem, back in [2003](https://www.researchgate.net/publication/4033100_Minimum_Redundancy_Feature_Selection_From_Microarray_Gene_Expression_Data). This makes it a minimal-optimal feature selection algorithm. Thus, I decided to quickly and efficiently select the top 10 relelveant features from my dataset."
      ],
      "metadata": {
        "id": "0F_jByL7DKX-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0l6Ngv7bod53",
        "outputId": "90bfb7dc-09cd-448e-f20c-ed50e245f520"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/smazzanti/mrmr\n",
            "  Cloning https://github.com/smazzanti/mrmr to /tmp/pip-req-build-atwk290e\n",
            "  Running command git clone -q https://github.com/smazzanti/mrmr /tmp/pip-req-build-atwk290e\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from mrmr==0.1) (2.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from mrmr==0.1) (4.62.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from mrmr==0.1) (1.1.0)\n",
            "Requirement already satisfied: pandas>=1.0.3 in /usr/local/lib/python3.7/dist-packages (from mrmr==0.1) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.7/dist-packages (from mrmr==0.1) (1.19.5)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from mrmr==0.1) (0.0)\n",
            "Requirement already satisfied: category_encoders in /usr/local/lib/python3.7/dist-packages (from mrmr==0.1) (2.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.3->mrmr==0.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.3->mrmr==0.1) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.0.3->mrmr==0.1) (1.15.0)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders->mrmr==0.1) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders->mrmr==0.1) (1.0.1)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders->mrmr==0.1) (0.10.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.7/dist-packages (from category_encoders->mrmr==0.1) (0.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->category_encoders->mrmr==0.1) (3.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->mrmr==0.1) (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "pip install git+https://github.com/smazzanti/mrmr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4ZyoJzron0F"
      },
      "outputs": [],
      "source": [
        "from mrmr import mrmr_classif\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "#creating data for dimensionality reduction\n",
        "X, y = make_classification(n_samples = 10000, n_features = 42, n_informative = 10, n_redundant = 32)\n",
        "X = mergedData\n",
        "y = mergedData['not_skipped'].squeeze()\n",
        "\n",
        "#using mrmr classification\n",
        "selected_features = mrmr_classif(X, y, K = 10)\n",
        "print(selected_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKrDO1XuHeYs"
      },
      "outputs": [],
      "source": [
        "#keeping selected features only in the dataframe\n",
        "mergedDataTemp = mergedData[mergedData.columns.difference(selected_features)]\n",
        "mergedDataReduced = mergedData[mergedData.columns.drop(mergedDataTemp)]\n",
        "mergedDataReduced.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeNl9m49DFmg"
      },
      "source": [
        "## Simple kNN implementation with reduced data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After successfully selecting the relevant set of features for the kNN implementation, I integrated Weights & Biases so I can use it for monitoring training process and hyperparameter tuning. Before training, the dataset had to be balanced for unbiased results."
      ],
      "metadata": {
        "id": "WDhLIR83lLJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install wandb"
      ],
      "metadata": {
        "id": "RAgY1NA3Yyy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "uXs3xnKkY3sC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ya-a7UFVm27u"
      },
      "outputs": [],
      "source": [
        "#balancing out dataset on skipped - not skipped songs\n",
        "balancedDataReduced = mergedDataReduced.groupby('not_skipped', group_keys=False).apply(lambda x: x.sample(1000000))\n",
        "balancedDataReduced.not_skipped.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_BtnRgJGSGt"
      },
      "outputs": [],
      "source": [
        "#splitting inputs and target variable\n",
        "X = balancedDataReduced.drop(columns = ['not_skipped']) #input\n",
        "y = balancedDataReduced['not_skipped'].values #target\n",
        "\n",
        "X.info()\n",
        "print(y[0:])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Machine Learning training, I utilised Scikit-learn library, offers straightforward framework for classificatioin tasks. To train my kNN model, there were variables which needed to be encoded into suitable, categorical formats."
      ],
      "metadata": {
        "id": "SUT2rEIbmcgN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLT98JPPUMfw"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "#encoding target variable to binary\n",
        "lb = LabelEncoder()\n",
        "y_encoded = lb.fit_transform(y)\n",
        "print(y_encoded)\n",
        "\n",
        "#converting string into categorical data\n",
        "X.session_id = pd.Categorical(X.session_id)\n",
        "X['session_id'] = X.session_id.cat.codes\n",
        "\n",
        "X.track_id = pd.Categorical(X.track_id)\n",
        "X['track_id'] = X.track_id.cat.codes\n",
        "\n",
        "X.hist_user_behavior_reason_start = pd.Categorical(X.hist_user_behavior_reason_start)\n",
        "X['hist_user_behavior_reason_start'] = X.hist_user_behavior_reason_start.cat.codes\n",
        "\n",
        "X.hist_user_behavior_reason_end = pd.Categorical(X.hist_user_behavior_reason_end)\n",
        "X['hist_user_behavior_reason_end'] = X.hist_user_behavior_reason_end.cat.codes\n",
        "\n",
        "#checking if datatypes are appropriate for training\n",
        "X.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viuU0JgvRQDM"
      },
      "outputs": [],
      "source": [
        "#splitting dataset between test and training \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.1, random_state=1, stratify=y)\n",
        "\n",
        "#creating KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors = 5)\n",
        "\n",
        "#fitting the classifier to the data\n",
        "knn.fit(X_train,y_train)\n",
        "\n",
        "#showing model predictions on the test data\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "#checking accuracy of model on the test data\n",
        "knn.score(X_test, y_test)\n",
        "\n",
        "y_probas = knn.predict_proba(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "knn.score(X_test, y_test)"
      ],
      "metadata": {
        "id": "FKCU95spctTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After training the kNN model on the dataset, it did not provide a sufficient accuracy, it performed only slightly better than a coin toss. However, the training did take time even with only 10 features, and using more complex sequential and vector based kNN [models](https://github.com/rn5l/session-rec/blob/master/algorithms/knn/vsknn.py) could have been even more computationally heavy. After some more research I found that for my task applying different classification models could be more applicable."
      ],
      "metadata": {
        "id": "BkUY5HLonqGp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ew6iJBMGIk5-"
      },
      "source": [
        "## Moving away from kNN implementation and using full dataset again"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After being inspired by [this](https://github.com/a-poor/spotify-skip-prediction/blob/master/lgbm_model_single_history.ipynb) notebook author's implementation, and studying decision trees more, I decided to move towards Gradient Boosted Trees. "
      ],
      "metadata": {
        "id": "0Vv4QWwOqI55"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature analysis with correlation matrices"
      ],
      "metadata": {
        "id": "dWLZry_goRLM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before getting into the design of the Gradient Boosted Trees, I wanted to find whether some features correlate with each other, and they could be used for dimensionality reduction. However, the dataset was still quite sparse in terms of correlations and as Gradient Boosted Trees are usually capable of handling high dimensinal data, I didn't actively continue on this path."
      ],
      "metadata": {
        "id": "tyEoJH2c4EY3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTrUIy_TIk6I"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16, 6))\n",
        "heatmap = sb.heatmap(mergedData[['not_skipped', \n",
        "                                   'session_position', \n",
        "                                   'session_length', \n",
        "                                   'track_id', \n",
        "                                   'context_switch', \n",
        "                                   'premium', \n",
        "                                   'skip_1', \n",
        "                                   'skip_2', \n",
        "                                   'skip_3',\n",
        "                                   'hist_user_behavior_is_shuffle',\n",
        "                                   'no_pause_before_play', \n",
        "                                   'short_pause_before_play',\n",
        "                                   'long_pause_before_play',\n",
        "                                   'hist_user_behavior_n_seekfwd',\n",
        "                                   'hist_user_behavior_n_seekback',\n",
        "                                   'hour_of_day',\n",
        "                                   'date',\n",
        "                                   'duration',\n",
        "                                   'release_year',\n",
        "                                   'us_popularity_estimate']].corr(), vmin=-1, vmax=1, annot=False);\n",
        "heatmap.set_title('Correlation Heatmap of user interaction logs', fontdict={'fontsize':15}, pad=12);\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16, 6))\n",
        "heatmap = sb.heatmap(mergedData[['not_skipped', 'acousticness', 'beat_strength', 'bounciness', 'danceability', 'dyn_range_mean',\n",
        "              'energy', 'flatness', 'instrumentalness', 'liveness', 'loudness',\n",
        "              'mechanism', 'organism', 'speechiness', 'tempo', 'valence']].corr(), vmin=-1, vmax=1, annot=False);\n",
        "heatmap.set_title('Correlation Heatmap of acoustic features', fontdict={'fontsize':15}, pad=12);\n"
      ],
      "metadata": {
        "id": "fJZ6tfyazZg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing previously cleaned data"
      ],
      "metadata": {
        "id": "xGCoGALbI3wV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For simpler handling of the dataset, I exported it to one .csv file and reexported it again.This .csv file can be found attached to the submission."
      ],
      "metadata": {
        "id": "kvxjgAeW7ATc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyTA8WTT0cd6"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/spotify_skip.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#displaying the first few rows\n",
        "data.head(5)"
      ],
      "metadata": {
        "id": "WGLbhvsN69aZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reformatting features' datatypes for optimal training performance"
      ],
      "metadata": {
        "id": "N_9LIQNYJAm-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly to the previous trial with kNN, I went through to reformat specific features, so they would be all suitable for training."
      ],
      "metadata": {
        "id": "dv3F_C4B7bdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#feature variable types\n",
        "\n",
        "#categorical\n",
        "cat_variable = ['mode', 'context_type', 'hist_user_behavior_reason_start', 'hist_user_behavior_reason_end']\n",
        "for c in cat_variable:\n",
        "    data[c] = data[c].astype('category')\n",
        "\n",
        "#boolean\n",
        "bool_variable = ['context_switch', 'no_pause_before_play', 'short_pause_before_play', 'long_pause_before_play',\n",
        "                'not_skipped', 'premium', 'hist_user_behavior_is_shuffle']\n",
        "for b in bool_variable:\n",
        "    data[b] = data[b].astype('bool')\n",
        "\n",
        "#ID\n",
        "id_variable = ['session_id', 'track_id', 'date']\n",
        "for i in id_variable:\n",
        "  le = LabelEncoder()\n",
        "  data[i] = le.fit_transform(data[i])\n",
        "\n",
        "data.head(30)"
      ],
      "metadata": {
        "id": "Iegud_h87w84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing sklearn libraries for training\n",
        "import lightgbm as lgbm\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import warnings"
      ],
      "metadata": {
        "id": "u2x664h58BLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reparsing data so skipped — not skipped ratio is balanced for training\n",
        "balancedData = data.groupby('not_skipped', group_keys=False).apply(lambda x: x.sample(1000000))\n",
        "\n",
        "balancedData.not_skipped.value_counts()"
      ],
      "metadata": {
        "id": "7vZlKGbd8EvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting features for training\n",
        "audio_features = ['duration', 'us_popularity_estimate', 'time_signature', 'key', 'mode', 'acousticness', 'beat_strength', 'bounciness', \n",
        "              'danceability', 'dyn_range_mean', 'energy', 'flatness', 'instrumentalness', 'liveness', 'loudness', 'mechanism', \n",
        "              'organism', 'speechiness', 'tempo', 'valence']\n",
        "\n",
        "user_interaction = ['context_switch', 'no_pause_before_play', 'short_pause_before_play', 'long_pause_before_play', \n",
        "                    'hist_user_behavior_n_seekfwd', 'hist_user_behavior_n_seekback', 'hist_user_behavior_is_shuffle', \n",
        "                    'hour_of_day', 'premium', 'context_type']"
      ],
      "metadata": {
        "id": "K0TrKicL8GRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training with Gradient Boosted Trees algorithm (LightGBM) — first audio features only"
      ],
      "metadata": {
        "id": "SqelRLJsto-I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient boosted trees can be classified as a type of decision tree algorithms, similary to Random Forests. Both of the algorithms combine multiple decision trees, so the implementation can avoid overfitting. While Random Forests use a method called bagging to combine these individual trees in parralel, Gradient Boosted Trees use a method named boosting. Instead of computing in parralel, trees which are weak learners (usually decision stumps) are sequentially combined. This is when boosting happens to correct the previous trees' errors. This is beneficial, as this approach has a great model capacity which yields in: faster training speed and higher efficiency, better accuracy, and capable of handling large-scale data. This is suitable for my dataset, however, gradient boosted trees are prone to overfitting. I used LightGBM, which is a gradient boosting [library](https://lightgbm.readthedocs.io/en/latest/index.html) for tree-based learning algorithms.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ijI_Abcu-Mwj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First I wanted to try out whether I can train a model solely based on audio features from the dataset (excluding user interaction data). Even though with this I did not expect high accuracy, I still managed to improve my previous kNN implementation."
      ],
      "metadata": {
        "id": "LxXZG8Oj9hFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#separating target variable (not_skipped) and input features from dataframe\n",
        "X_input = balancedData[audio_features] #first training only on audio characters\n",
        "y_target = balancedData['not_skipped']"
      ],
      "metadata": {
        "id": "CzH5ociE8SZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting dataframe between train, validation, and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_input, y_target, train_size=0.9)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, train_size=0.8)\n",
        "\n",
        "X_train.shape, X_val.shape, X_test.shape"
      ],
      "metadata": {
        "id": "vset3wCr8UjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing weights & biases project\n",
        "!pip install wandb\n",
        "import wandb\n",
        "\n",
        "wandb.init(project=\"spotify_skip_predict\", entity=\"101010\")"
      ],
      "metadata": {
        "id": "RHVY_T8RNy4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training light gradient boosted tree model on audio features only\n",
        "model = lgbm.LGBMClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_val)\n",
        "y_probas = model.predict_proba(X_test)\n",
        "importances = model.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "print('Accuracy: {:.3f}'.format(accuracy_score(y_val, y_pred)))"
      ],
      "metadata": {
        "id": "rRaGJ-o28bVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training with user interactions and audio features — all the previously selected features"
      ],
      "metadata": {
        "id": "biHBdx67NlrE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the whole dataset this time for training:"
      ],
      "metadata": {
        "id": "s6cYF2J3DYqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#separating target variable (not_skipped) and input features from dataframe\n",
        "X_input = balancedData[audio_features + user_interaction] #training on all the features\n",
        "y_target = balancedData['not_skipped']"
      ],
      "metadata": {
        "id": "TcnCuqhJuBtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting dataframe between train, validation, and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_input, y_target, train_size=0.9)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, train_size=0.8)\n",
        "\n",
        "X_train.shape, X_val.shape, X_test.shape"
      ],
      "metadata": {
        "id": "cWYvY0SjuYfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training light gradient boosted tree model on all features\n",
        "model = lgbm.LGBMClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_val)\n",
        "y_probas = model.predict_proba(X_test)\n",
        "importances = model.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "print('Accuracy: {:.3f}'.format(accuracy_score(y_val, y_pred)))"
      ],
      "metadata": {
        "id": "srIRAJjUuYVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter tuning with Weights & Biases"
      ],
      "metadata": {
        "id": "lZAfQPKIFwbz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As training on the whole dataset with LightGBM did not improve drastically the model's accuracy, I decided to do hyperparameter tuning with Weights & Biases. Hyperparameter tuning enables to select the most optimal hyperparameters for the training model itslef, for achieving the best accuracy. For the optimisation process I used grid search, which is one of the most traditional ways of hyperparamter tuning. While balancing between achieving sufficient accuracy and avoiding overfitting, I found that based on my [research](https://towardsdatascience.com/hyperparameter-tuning-to-reduce-overfitting-lightgbm-5eb81a0b464e) the selected parameters can aid the training process."
      ],
      "metadata": {
        "id": "V9RieuQQDx9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#grid search sweep for hyperparameter tuning\n",
        "sweep_config = {\n",
        "    \"name\": \"audio_user_feat_eng_2\",\n",
        "    \"method\": \"grid\",\n",
        "    \"metric\": {\n",
        "        \"name\": \"accuracy\",\n",
        "        \"goal\": \"maximize\"\n",
        "        },\n",
        "    \"parameters\": {\n",
        "        \"max_depth\": {\n",
        "            \"values\": [3, 5, 10, 15]\n",
        "        },\n",
        "        \"num_leaves\": {\n",
        "            \"values\": [10, 20, 30]\n",
        "        },\n",
        "        \"learning_rate\": {\n",
        "            \"values\": [.05, .1, .2]\n",
        "        },\n",
        "        \"subsample\": {\n",
        "            \"values\": [1, .8, .5]\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "xFvD7iCH9_wY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    config_defaults = {\n",
        "      \"max_depth\": 3,\n",
        "      \"num_leaves\": 10,\n",
        "      \"learning_rate\": .05,\n",
        "      \"subsample\": 1,\n",
        "    }\n",
        "    wandb.init(project=\"spotify_skip_predict\", entity=\"101010\", config=config_defaults)\n",
        "    config = wandb.config\n",
        "\n",
        "    X_data = balancedData[audio_features + user_interaction]\n",
        "    Y_data = balancedData['not_skipped']\n",
        "\n",
        "    #splitting data into train, validation, and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_data, Y_data, train_size=0.9)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, train_size=0.8)\n",
        "\n",
        "    #fitting model on training set\n",
        "    model = lgbm.LGBMClassifier(max_depth=config.max_depth,\n",
        "                                num_leaves=config.num_leaves,\n",
        "                                learning_rate=config.learning_rate, \n",
        "                                subsample=config.subsample)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    #making predictions on test set\n",
        "    y_pred = model.predict(X_val)\n",
        "\n",
        "    #evaluating predictions\n",
        "    accuracy = accuracy_score(y_val, y_pred)\n",
        "    print(f\"Accuracy: {int(accuracy * 100.)}%\")\n",
        "    wandb.log({\"accuracy\": accuracy})"
      ],
      "metadata": {
        "id": "cGPhqi4z42WB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep_config, entity=\"101010\", project=\"spotify_skip_predict\")"
      ],
      "metadata": {
        "id": "pjzi1nRG_stp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.agent(sweep_id, train)"
      ],
      "metadata": {
        "id": "NS9m9BVT_8-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I managed to increase test accuracy minimally through hyperparameter tuning from 60.6% to 60.87%. Here the parralel coordinates plot and the accuracy scores can be seen:"
      ],
      "metadata": {
        "id": "mngoc7Z-9ffb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://wandb.ai/101010/spotify_skip_predict/reports/Hyperparameter-tuning--VmlldzoxMzg4NzI5?accessToken=807v08fdor2kc9cwnd2btjn17rin98q21pob5ql65oo99qfrrlckx2i3m6o8932k\n"
      ],
      "metadata": {
        "id": "_nATtzTkJF4E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding extra features via feature engineering"
      ],
      "metadata": {
        "id": "cloyv-vs-6H6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the hypermarameter tuning only minimally improved the test accuracy, I wanted to add extra features via feature engineering. To link back to the original dataset's nature, I wanted to take into account music streaming's sequential manner. Implementing a more sequence-aware feature could increase the accuracy. Thus, I decided to two new features to the dataset, if the previous song was skipped or not, and the previous track's length."
      ],
      "metadata": {
        "id": "w1kzXg8vF58p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#adding if previous track was skipped or not\n",
        "data['prev_not_skipped'] = data.groupby(['session_id'])['not_skipped'].shift(1)\n",
        "data['prev_not_skipped'] = data['prev_not_skipped'].astype('bool')\n",
        "\n",
        "#adding the previous track's length\n",
        "data['prev_duration'] = data.groupby(['session_id'])['duration'].shift(1)"
      ],
      "metadata": {
        "id": "wH3WAk_b-x-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "engineered_features = ['prev_not_skipped', 'prev_duration', 'session_position']\n",
        "\n",
        "X_data = data[audio_features + user_interaction + engineered_features]\n",
        "Y_data = data['not_skipped']"
      ],
      "metadata": {
        "id": "Q121DB6k-3fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_data, Y_data, train_size=0.9)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, train_size=0.8)\n",
        "\n",
        "X_train.shape, X_val.shape, X_test.shape"
      ],
      "metadata": {
        "id": "IrMft_Ma_rEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#applying previously tuned hyperparameters\n",
        "model = lgbm.LGBMClassifier(\n",
        "    max_depth = 10,\n",
        "    num_leaves = 20,\n",
        "    learning_rate = .2,\n",
        "    subsample = 0.8\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_val)\n",
        "print('Accuracy: {:.3f}'.format(accuracy_score(y_val, y_pred)))"
      ],
      "metadata": {
        "id": "5vweWJNJ_wmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After testing the model with the new features and the optimised hyperparameteres, I managed to achieve 80.6% accuracy. This can be considered as a decent result given the scope of the problem, and the [leaderboard](https://www.aicrowd.com/challenges/spotify-sequential-skip-prediction-challenge/leaderboards) at the skip prediction challange.\n",
        "\n",
        "However, I believe that my implementation can be still somewhat overfitting, which can be found out after another round of hyperparameter tuning, and a more detailed evaluation of the results."
      ],
      "metadata": {
        "id": "JUdF3OaSHnhI"
      }
    }
  ]
}